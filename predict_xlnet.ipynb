{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_xlnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiwJP_niHR4V"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload model(model.zip)"
      ],
      "metadata": {
        "id": "MsL2CUxkJtUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "impb58rlISk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip"
      ],
      "metadata": {
        "id": "WqmPTuUhJzZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip model.zip"
      ],
      "metadata": {
        "id": "f3X1xMe8IQfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "import torch\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = XLNetTokenizer.from_pretrained('./model')\n",
        "# model = BertForSequenceClassification.from_pretrained('./model',problem_type=\"multi_label_classification\")\n",
        "model = XLNetForSequenceClassification.from_pretrained('./model')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "3ASrSw9wHuAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare predict function"
      ],
      "metadata": {
        "id": "4sitGdvdJ_Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "import torch.nn.functional as F\n",
        "#this will predict one sentence each time\n",
        "def predict(content):\n",
        "\n",
        "    inputs = tokenizer(content,\n",
        "                       \n",
        "                       padding='max_length',\n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "    # move to gpu\n",
        "    ids = inputs[\"input_ids\"].to(device)\n",
        "    idt = inputs[\"token_type_ids\"].to(device)\n",
        "    mask = inputs[\"attention_mask\"].to(device)\n",
        "    # forward pass\n",
        "    outputs = model(ids,token_type_ids=idt,attention_mask=mask)\n",
        "    logits = outputs[0]\n",
        "    x = F.softmax(logits, dim=-1)\n",
        "    active_logits = logits.view(-1, model.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
        "    flattened_predictions = torch.argmax(active_logits,\n",
        "                                         axis=1)\n",
        "    return x.cpu().detach().numpy()[0][1], flattened_predictions.cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "ToiwTGLuJB56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload test data, still tr.csv\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "-FPc2J8IJB_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare test data"
      ],
      "metadata": {
        "id": "CiliqftyKapu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"tr.csv\")"
      ],
      "metadata": {
        "id": "oWo-_I4KKYcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df.irsen_text.values.tolist()[7000:]\n",
        "real = df.claim_s.values.tolist()[7000:]\n",
        "len(sentences)"
      ],
      "metadata": {
        "id": "oxY5jGEOKfMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict the test set"
      ],
      "metadata": {
        "id": "JWzl1KdcKPfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre = []# predict label\n",
        "pre_pro = []# predict probility\n",
        "for i in sentences:\n",
        "  x = predict(i)\n",
        "  pre.append(x[1])\n",
        "  pre_pro.append(x[0])"
      ],
      "metadata": {
        "id": "vWkSBkhRKKgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show result"
      ],
      "metadata": {
        "id": "QawAAO9hKmy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('f1:'+str(f1_score(real, pre, average=None))+'\\n'+'recall:'+str(recall_score(real, pre, average=None))+'\\n'+'precision:'+str(precision_score(real, pre, average=None))+'\\n'+'accuracy:'+str(accuracy_score(real, pre))+'\\n')"
      ],
      "metadata": {
        "id": "_2RmnTp1KKng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(real, pre)"
      ],
      "metadata": {
        "id": "MlcIp3SRKKtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fig, ax = plt.subplots(1,1,figsize = (10,5))\n",
        "ax.hist(np.array(pre_pro)[np.array(real) == 1], color = \"darkred\",bins = \"scott\", alpha = .5, edgecolor = \"red\")\n",
        "ax.hist(np.array(pre_pro)[np.array(real) == 0], color = \"darkgreen\",bins = \"scott\", alpha = .5, edgecolor = \"green\")"
      ],
      "metadata": {
        "id": "Sn-_yiwmKKw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of samples model gives probabilty more than .8 but real label are non-evidence\n",
        "print('num of samples have score more than 0.8 but are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.8)))\n",
        "print('num of samples have score more than 0.8 are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.8)))\n",
        "# how much confidence if the socore is higher than 0.8, we are 85.7% confident that the sentence is evidence if the score is higher than .8\n",
        "confi80 = str(round((np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.8))/(np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.8) + np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.8)),4))\n",
        "print('we are '+ confi80 +' confident that the sentence is evidence if the score is higher than .8')\n",
        "# number of samples model gives probabilty more than .6 less than .8 but real label is non-evidence\n",
        "num6080_nevid = np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.6) - np.sum(np.array(pre_pro)[np.array(real) == 0]>=0.80)\n",
        "num6080_evid = np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.6) - np.sum(np.array(pre_pro)[np.array(real) == 1]>=0.80)\n",
        "print('num of samples have score more than 0.6 and less than 0.8 but are non-evidence :'+ str(num6080_nevid))\n",
        "print('num of samples have score more than 0.6 and less than 0.8 are evidence :'+ str(num6080_evid))\n",
        "confi6080 = num6080_evid/(num6080_evid + num6080_nevid)\n",
        "print('we are '+ str(round(confi6080,4)) +' confident that the sentence is evidence if the score is higher than .6 and less than .8')"
      ],
      "metadata": {
        "id": "UbUtnT7RKKzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('num of samples have score more than 0.8 but are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2)))\n",
        "print('num of samples have score more than 0.8 are non-evidence : '+ str(np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.2)))\n",
        "confi20 = str(round((np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2))/(np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2) + np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.2)),4))\n",
        "print('we are '+ confi80 +' confident that the sentence is non-evidence if the score is higher than .2')\n",
        "num2040_nevid = np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.4) - np.sum(np.array(pre_pro)[np.array(real) == 0]<=0.2)\n",
        "num2040_evid = np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.4) - np.sum(np.array(pre_pro)[np.array(real) == 1]<=0.2)\n",
        "print('num of samples have score more than 0.2 and less than 0.4 but are non-evidence :'+ str(num2040_nevid))\n",
        "print('num of samples have score more than 0.2 and less than 0.4 are evidence :'+ str(num2040_evid))\n",
        "confi2040 = num2040_nevid/(num2040_evid + num2040_nevid)\n",
        "print('we are '+ str(round(confi2040,4)) +' confident that the sentence is non-evidence if the score is higher than .2 and less than .4')"
      ],
      "metadata": {
        "id": "pKaAtwi-KwXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the wrong predictions. The end of sentence shows the real labels\n",
        "for i in range(len(real)):\n",
        "  if real[i] != pre[i]:\n",
        "    print(sentences[i] + ' claim' if real[i] else sentences[i] + ' noclaim')"
      ],
      "metadata": {
        "id": "siuPqsslK0nx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}